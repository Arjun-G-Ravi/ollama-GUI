i want a localhost based app using flask, html, css for running llms locally which have to provide a ui over ollama. i run locally using ollama. this should properly see all the models that ive downloaded and show that in the UI. Also, i should be able to save multiple default prompts, which i should be able to reuse using a click. make teh UI look great


features needed
- show gpu, cpu usage
- show ram, vram usage (it getting filled up as we load models)
- stream the results
- the UI should support codeblocks(with python color for LSP and all)
- UI should support rendering tables
- refresh talk and clear context button at top right
- store conversation history
- ability to edit/ delete saved prompts


- i want the gpucpu info in 2 boxes. 
    - first shows cpu % and RAM in numbers(7.6/16GB)
    - second shows gpu% and VRAM in numbers(1.2/4GB)
    - this should update very fast (every 0.25 seconds)

- the history is saving every talk and back in full conversation as separate history. one conversation needs to be in one history block only
- one button next to clear context to pkill ollama
- add copy button for codeblocks
- When you are loading any model, say loaing model
- tell the tokens per second for conversation as small text 